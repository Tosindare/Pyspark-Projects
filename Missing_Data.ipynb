{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python [conda root]",
      "language": "python",
      "name": "conda-root-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3"
    },
    "colab": {
      "name": "Missing_Data.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QBDuXIPTRDi",
        "outputId": "710651ba-d4df-45c7-ce2f-b7cfd541d045"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB)\n",
            "\u001b[K     |████████████████████████████████| 204.2MB 71kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 47.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612242 sha256=1d101fbcd233b397f13a6306b917578c8e257690415a2764cbf13df0da600509\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqWL5WOeRcvG"
      },
      "source": [
        "# Missing Data\n",
        "\n",
        "Dealing with missing data: \n",
        "3 basic options decisions as to be made for the right approach:\n",
        "\n",
        "* Just keep the missing data points.\n",
        "* Drop them missing data points (including the entire row)\n",
        "* Fill them in with some other value.\n",
        "\n",
        "Examples..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "yCmuvbekRcvP"
      },
      "source": [
        "## Keep the missing data\n",
        "A few machine learning algorithms can easily deal with missing data as follows;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "GxscHmWtRcvQ"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# May take a little while on a local computer\n",
        "spark = SparkSession.builder.appName(\"missingdata\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "wgyU0_jKRcvR"
      },
      "source": [
        "df = spark.read.csv(\"Missing_Data.csv\",header=True,inferSchema=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRqLAekPRcvS",
        "outputId": "4644b8ef-cc82-4768-ade3-108dc38122ff"
      },
      "source": [
        "df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-----+-----+\n",
            "|  Id| Name|Sales|\n",
            "+----+-----+-----+\n",
            "|emp1| John| null|\n",
            "|emp2| null| null|\n",
            "|emp3| null|345.0|\n",
            "|emp4|Cindy|456.0|\n",
            "+----+-----+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPjZlgVZRcvT"
      },
      "source": [
        "Notice how the data remains as a null."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXwIL2qqRcvU"
      },
      "source": [
        "## Drop the missing data\n",
        "\n",
        "You can use the .na functions for missing data. The drop command has the following parameters:\n",
        "\n",
        "    df.na.drop(how='any', thresh=None, subset=None)\n",
        "    \n",
        "    * param how: 'any' or 'all'.\n",
        "    \n",
        "        If 'any', drop a row if it contains any nulls.\n",
        "        If 'all', drop a row only if all its values are null.\n",
        "    \n",
        "    * param thresh: int, default None\n",
        "    \n",
        "        If specified, drop rows that have less than `thresh` non-null values.\n",
        "        This overwrites the `how` parameter.\n",
        "        \n",
        "    * param subset: \n",
        "        optional list of column names to consider."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "St3yNaaaRcvU",
        "outputId": "d6173bac-7e38-48ef-ca08-61c7e2b06ece"
      },
      "source": [
        "# Drop any row that contains missing data\n",
        "df.na.drop().show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-----+-----+\n",
            "|  Id| Name|Sales|\n",
            "+----+-----+-----+\n",
            "|emp4|Cindy|456.0|\n",
            "+----+-----+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKtoHAMnRcvV",
        "outputId": "ecc084d2-ec67-4efb-902e-594122acdd30"
      },
      "source": [
        "# Has to have at least 2 NON-null values\n",
        "df.na.drop(thresh=2).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-----+-----+\n",
            "|  Id| Name|Sales|\n",
            "+----+-----+-----+\n",
            "|emp1| John| null|\n",
            "|emp3| null|345.0|\n",
            "|emp4|Cindy|456.0|\n",
            "+----+-----+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mtZWvXlRcvV",
        "outputId": "88f37cf5-9f61-4bab-9710-d49f679a8159"
      },
      "source": [
        "df.na.drop(subset=[\"Sales\"]).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-----+-----+\n",
            "|  Id| Name|Sales|\n",
            "+----+-----+-----+\n",
            "|emp3| null|345.0|\n",
            "|emp4|Cindy|456.0|\n",
            "+----+-----+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8oLr92hRcvV",
        "outputId": "ccae44c8-237f-4280-d093-703ad76c759d"
      },
      "source": [
        "df.na.drop(how='any').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-----+-----+\n",
            "|  Id| Name|Sales|\n",
            "+----+-----+-----+\n",
            "|emp4|Cindy|456.0|\n",
            "+----+-----+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxuSkHdpRcvW",
        "outputId": "f9fc4f5a-faae-4ece-8cc1-d4e195d297a4"
      },
      "source": [
        "df.na.drop(how='all').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-----+-----+\n",
            "|  Id| Name|Sales|\n",
            "+----+-----+-----+\n",
            "|emp1| John| null|\n",
            "|emp2| null| null|\n",
            "|emp3| null|345.0|\n",
            "|emp4|Cindy|456.0|\n",
            "+----+-----+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1pK6DU1RcvW"
      },
      "source": [
        "## Fill the missing values\n",
        "\n",
        "We can also fill the missing values with new values. If you have multiple nulls across multiple data types, Spark is actually smart enough to match up the data types. For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLU9tt-7RcvX",
        "outputId": "eb5b3877-93f3-47af-a678-b831a16a6d58"
      },
      "source": [
        "df.na.fill('NEW VALUE').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+---------+-----+\n",
            "|  Id|     Name|Sales|\n",
            "+----+---------+-----+\n",
            "|emp1|     John| null|\n",
            "|emp2|NEW VALUE| null|\n",
            "|emp3|NEW VALUE|345.0|\n",
            "|emp4|    Cindy|456.0|\n",
            "+----+---------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWQMQr_HRcvX",
        "outputId": "620332ba-7ddd-4908-e13d-4be967ef93d4"
      },
      "source": [
        "df.na.fill(0).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-----+-----+\n",
            "|  Id| Name|Sales|\n",
            "+----+-----+-----+\n",
            "|emp1| John|  0.0|\n",
            "|emp2| null|  0.0|\n",
            "|emp3| null|345.0|\n",
            "|emp4|Cindy|456.0|\n",
            "+----+-----+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWFqDauxRcvY"
      },
      "source": [
        "Usually you should specify what columns you want to fill with the subset parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFruWICLRcvY",
        "outputId": "ed81c5ae-92e4-45c3-e06e-2463cdfea4b0"
      },
      "source": [
        "df.na.fill('No Name',subset=['Name']).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-------+-----+\n",
            "|  Id|   Name|Sales|\n",
            "+----+-------+-----+\n",
            "|emp1|   John| null|\n",
            "|emp2|No Name| null|\n",
            "|emp3|No Name|345.0|\n",
            "|emp4|  Cindy|456.0|\n",
            "+----+-------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nD2YFaIRcvY"
      },
      "source": [
        "A very common practice is to fill values with the mean value for the column, for example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qucPnBuVRcvZ",
        "outputId": "e86a4564-2b7a-45d1-b149-e154d2ac84e3"
      },
      "source": [
        "from pyspark.sql.functions import mean\n",
        "mean_val = df.select(mean(df['Sales'])).collect()\n",
        "\n",
        "# Weird nested formatting of Row object!\n",
        "mean_val[0][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400.5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "5UM_A_T2RcvZ"
      },
      "source": [
        "mean_sales = mean_val[0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4f1J9-FRcva",
        "outputId": "fc2c0ae7-80a6-40ca-d3bc-7946fa2212ea"
      },
      "source": [
        "df.na.fill(mean_sales,[\"Sales\"]).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-----+-----+\n",
            "|  Id| Name|Sales|\n",
            "+----+-----+-----+\n",
            "|emp1| John|400.5|\n",
            "|emp2| null|400.5|\n",
            "|emp3| null|345.0|\n",
            "|emp4|Cindy|456.0|\n",
            "+----+-----+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Io07rop7Rcva",
        "outputId": "f25e3118-ae0c-4652-c1d5-d7769282f529"
      },
      "source": [
        "# One (very ugly) one-liner\n",
        "df.na.fill(df.select(mean(df['Sales'])).collect()[0][0],['Sales']).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-----+-----+\n",
            "|  Id| Name|Sales|\n",
            "+----+-----+-----+\n",
            "|emp1| John|400.5|\n",
            "|emp2| null|400.5|\n",
            "|emp3| null|345.0|\n",
            "|emp4|Cindy|456.0|\n",
            "+----+-----+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgSm5tzrRcva"
      },
      "source": [
        "That is all we need to know for now!"
      ]
    }
  ]
}